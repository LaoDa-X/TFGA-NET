{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "424e5cb8",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43c85aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:18.613901Z",
     "start_time": "2025-08-09T03:24:16.394482Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import json\n",
    "import time \n",
    "import requests\n",
    "import random\n",
    "import functools\n",
    "import ipykernel\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import copy, math\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import rearrange\n",
    "from torchaudio import transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from math import cos, pi, floor, sin\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from sacred import Experiment\n",
    "from sacred.observers import FileStorageObserver\n",
    "from sacred.utils import apply_backspaces_and_linefeeds\n",
    "import gc\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "from torch import einsum\n",
    "from typing import Optional\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import Tensor\n",
    "import torch.nn.init as init\n",
    "from pprint import pprint\n",
    "from types import SimpleNamespace \n",
    "from typing import List, Tuple\n",
    "from scipy.signal import butter, lfilter, sosfilt\n",
    "from pesq import pesq\n",
    "from pystoi import stoi  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d266e2f",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a159f6",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc3a08e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:19.013319Z",
     "start_time": "2025-08-09T03:24:18.623103Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    root = ''\n",
    "    file_name = ''\n",
    "    root_feature = ''\n",
    "    \n",
    "    sample_rate = 16000                     \n",
    "    learning_rate = 1e-4\n",
    "    batch_size = 8\n",
    "    \n",
    "    SUBJECTS = []\n",
    "    seed=42\n",
    "    \n",
    "    #Mossformer2\n",
    "    #FLASHT\n",
    "    emb_size = 128                          \n",
    "    encoder_kernel_size = 20               \n",
    "    encoder_out_nchannels = 256           \n",
    "    intra_numlayers = 4                   \n",
    "    intra_nhead = 8                        \n",
    "    intra_dffn = 512                       \n",
    "    intra_dropout = 0.1                    \n",
    "    intra_use_positional = True            \n",
    "    intra_norm_before = True              \n",
    "\n",
    "    #MASK\n",
    "    encoder_out_nchannels = 256             \n",
    "    masknet_numspks = 1                   \n",
    "    masknet_chunksize = 100                 \n",
    "    masknet_numlayers = 6                   \n",
    "    masknet_norm = \"ln\"                     \n",
    "    masknet_useextralinearlayer = False     \n",
    "    masknet_extraskipconnection = True        \n",
    "              \n",
    "    #Encoder\n",
    "    num_channels = 64\n",
    "    sequence_length = 128\n",
    "    num_subjects = 32\n",
    "    num_features = 128\n",
    "    num_latents = 256\n",
    "    num_blocks = 1\n",
    "\n",
    "    chan_num = 128   \n",
    "    band_num = 5   \n",
    "    inc = 32      \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d431c59-6444-456f-b13e-461e134f52bd",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c71a8550",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:34.011373Z",
     "start_time": "2025-08-09T03:24:33.896529Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.sum += value * n\n",
    "        self.count += n\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.sum / self.count\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5719a369",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ee1a0b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:34.853311Z",
     "start_time": "2025-08-09T03:24:34.447604Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(net, X, mrstftloss, **kwargs):\n",
    "    \"\"\"\n",
    "    Loss function in CleanUNet\n",
    "\n",
    "    Parameters:\n",
    "    net: network\n",
    "    X: training data pair (clean audio, noisy_audio)\n",
    "    ell_p: \\ell_p norm (1 or 2) of the AE loss\n",
    "    ell_p_lambda: factor of the AE loss\n",
    "    stft_lambda: factor of the STFT loss\n",
    "    mrstftloss: multi-resolution STFT loss function\n",
    "\n",
    "    Returns:\n",
    "    loss: value of objective function\n",
    "    output_dic: values of each component of loss\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(X) == tuple and len(X) == 4\n",
    "    \n",
    "    noisy_audio, eeg, clean_audio,eeg_feature = X\n",
    "    loss = 0.0\n",
    "    denoised_audio = net(noisy_audio, eeg,eeg_feature)\n",
    "    \n",
    "    sc_loss = mrstftloss(denoised_audio.squeeze(1), clean_audio.squeeze(1))\n",
    "\n",
    "    loss += sc_loss\n",
    "\n",
    "    return loss,denoised_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ed2be05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:34.972839Z",
     "start_time": "2025-08-09T03:24:34.855556Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_SISDR(estimate_source, source):\n",
    "    \"\"\"Calculate Scale-Invariant Source-to-Distortion Ratio (SI-SDR)\n",
    "    Args:\n",
    "        source: torch tensor, [batch size, sequence length]\n",
    "        estimate_source: torch tensor, [batch size, sequence length]\n",
    "    Returns:\n",
    "        SISDR, [batch size]\n",
    "    \"\"\"\n",
    "    source = source.squeeze()\n",
    "    estimate_source = estimate_source.squeeze()\n",
    "    assert source.size() == estimate_source.size()\n",
    "    \n",
    "    # Step 1. Zero-mean norm (optional for SI-SDR, but keeping consistent with SI-SNR)\n",
    "    source = source - torch.mean(source, axis=-1, keepdim=True)\n",
    "    estimate_source = estimate_source - torch.mean(estimate_source, axis=-1, keepdim=True)\n",
    "    \n",
    "    # Step 2. SI-SDR calculation\n",
    "    EPS = 1e-9  # Small value to avoid division by zero\n",
    "    \n",
    "    # Calculate scaling factor alpha = <s',s> / <s,s>\n",
    "    source_energy = torch.sum(source * source, axis=-1, keepdim=True) + EPS\n",
    "    scaling = torch.sum(source * estimate_source, axis=-1, keepdim=True) / source_energy\n",
    "    \n",
    "    # s_target = alpha * s\n",
    "    s_target = scaling * source\n",
    "    # e_noise = s' - s_target\n",
    "    noise = estimate_source - s_target\n",
    "    \n",
    "    # SI-SDR = 10 * log_10(||s_target||^2 / ||e_noise||^2)\n",
    "    target_energy = torch.sum(s_target ** 2, axis=-1)\n",
    "    noise_energy = torch.sum(noise ** 2, axis=-1) + EPS\n",
    "    ratio = target_energy / noise_energy\n",
    "    \n",
    "    sisdr = 10 * torch.log10(ratio + EPS)\n",
    "    return sisdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2382e54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:35.103065Z",
     "start_time": "2025-08-09T03:24:34.974929Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class si_sidrloss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"Calculate SI-SDR loss\n",
    "        Args:\n",
    "            pred: torch tensor, [batch size, sequence length]\n",
    "            target: torch tensor, [batch size, sequence length]\n",
    "        Returns:\n",
    "            loss: torch tensor, scalar\n",
    "        \"\"\"\n",
    "        sisdr = calc_SISDR(pred, target)\n",
    "        # Return negative mean SI-SDR as loss (higher SI-SDR is better, so negate for loss)\n",
    "        return -torch.mean(sisdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ad5f5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:35.221313Z",
     "start_time": "2025-08-09T03:24:35.105173Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def cal_sisdri(mix_wave, target_wave, estmate_wave):\n",
    "    sisdr1 = calc_SISDR(mix_wave, target_wave)\n",
    "    sisdr2 = calc_SISDR(estmate_wave, target_wave)\n",
    "    sisdri = sisdr2 - sisdr1\n",
    "    \n",
    "    return sisdri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135886ad",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4699bb30",
   "metadata": {},
   "source": [
    "## EEGEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06233bcd-4eda-4ab0-b1c4-1aa4238a537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class down_sample(nn.Module):\n",
    "    def __init__(self, c, k, s, p):\n",
    "        super().__init__()\n",
    "        self.op = nn.Sequential(\n",
    "            nn.Conv2d(c, c, (1, k), stride=(1, s), padding=(0, p), bias=False),\n",
    "            nn.BatchNorm2d(c),\n",
    "            nn.ELU(False)\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.op[0].weight)\n",
    "    def forward(self, x): return self.op(x)\n",
    "\n",
    "class Residual_Block(nn.Module):\n",
    "    def __init__(self, inc, outc, g=1):\n",
    "        super().__init__()\n",
    "        self.exp   = nn.Conv2d(inc, outc, 1, bias=False) if inc != outc else None\n",
    "        self.conv1 = nn.Conv2d(inc,  outc, (1,3), padding=(0,1), groups=g, bias=False)\n",
    "        self.conv2 = nn.Conv2d(outc, outc, (1,3), padding=(0,1), groups=g, bias=False)\n",
    "        self.bn1, self.bn2 = nn.BatchNorm2d(outc), nn.BatchNorm2d(outc)\n",
    "        self.elu = nn.ELU(False)\n",
    "        for m in (self.conv1, self.conv2):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    def forward(self, x):\n",
    "        y = self.bn1(self.conv1(x))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        return self.elu(y + (self.exp(x) if self.exp is not None else x))\n",
    "\n",
    "class input_layer(nn.Module):\n",
    "    def __init__(self, outc, g=1):\n",
    "        super().__init__()\n",
    "        self.op = nn.Sequential(\n",
    "            nn.Conv2d(1, outc, (1,3), padding=(0,1), groups=g, bias=False),\n",
    "            nn.BatchNorm2d(outc)\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.op[0].weight)\n",
    "    def forward(self, x): return self.op(x)\n",
    "\n",
    "def embedding_network(outc, nlayer, g=1):\n",
    "    layers = [input_layer(outc, g)]\n",
    "    for i in range(nlayer):\n",
    "        layers.append(Residual_Block(int(2**i * outc),\n",
    "                                     int(2**(i+1) * outc), g))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# ============================================================\n",
    "#                     Temporal Branch\n",
    "# ============================================================\n",
    "class Multi_Scale_Temporal_Block(nn.Module):\n",
    "    def __init__(self, outc, nlayer=1):\n",
    "        super().__init__()\n",
    "        self.embed = embedding_network(outc, nlayer, 1)\n",
    "        ch = outc * (2**nlayer) + 1\n",
    "        self.ds1 = down_sample(ch,  4,  2, 1)\n",
    "        self.ds2 = down_sample(ch,  8,  4, 2)\n",
    "        self.ds3 = down_sample(ch, 16,  8, 4)\n",
    "        self.ds4 = down_sample(ch, 16, 16, 0)\n",
    "        self.ds5 = down_sample(ch, 16, 16, 0)\n",
    "    def forward(self,x):\n",
    "        cat = torch.cat((self.embed(x), x),1)\n",
    "        return torch.concat((self.ds1(cat), self.ds2(cat),\n",
    "                             self.ds3(cat), self.ds4(cat), self.ds5(cat)), 3)\n",
    "\n",
    "class Temporal_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Multi_Scale_Temporal_Block(2) for _ in range(5)])\n",
    "        self.up = nn.Identity()\n",
    "    def forward(self,x):\n",
    "        outs = [blk(x[:, i:i+1]) for i, blk in enumerate(self.blocks)]\n",
    "        return self.up(torch.cat(outs,1))               # [B,25,128,256]\n",
    "\n",
    "# ============================================================\n",
    "#                     GCN \n",
    "# ============================================================\n",
    "class Electrodes:\n",
    "    \"\"\"128 Electrode coordinates + learnable edge importance\"\"\"\n",
    "    def __init__(self):\n",
    "        # ---------- 128×3  ----------\n",
    "        self.positions_3d = np.array([\n",
    "            # A1–A32 --------------------------------------------------\n",
    "            [ 0.000000,  0.095000,  0.000000], [-0.012383,  0.093504,  0.011344],\n",
    "            [ 0.002068,  0.092008, -0.023564], [ 0.017557,  0.090512,  0.022899],\n",
    "            [-0.032677,  0.089016, -0.005780], [ 0.031177,  0.087520, -0.019832],\n",
    "            [-0.010465,  0.086024,  0.038928], [-0.019985,  0.084528, -0.038480],\n",
    "            [ 0.043359,  0.083031,  0.015835], [-0.045066,  0.081535,  0.018602],\n",
    "            [ 0.021690,  0.080039, -0.046349], [ 0.015994,  0.078543,  0.050992],\n",
    "            [-0.048085,  0.077047, -0.027866], [ 0.056250,  0.075551, -0.012366],\n",
    "            [-0.034223,  0.074055,  0.048679], [-0.007880,  0.072559, -0.060812],\n",
    "            [ 0.048231,  0.071063,  0.036332], [ 0.002143,  0.069567,  0.065043],\n",
    "            [-0.059341,  0.068071, -0.001459], [ 0.062635,  0.066575,  0.004402],\n",
    "            [-0.016635,  0.065079, -0.070415], [-0.028852,  0.063583,  0.065405],\n",
    "            [ 0.055184,  0.062087, -0.040042], [-0.070070,  0.060591,  0.019264],\n",
    "            [ 0.032360,  0.059095,  0.062903], [ 0.010572,  0.057599, -0.074345],\n",
    "            [-0.050861,  0.056103,  0.055513], [ 0.074536,  0.054607, -0.018668],\n",
    "            [-0.030532,  0.053111, -0.071740], [-0.002514,  0.051615,  0.078259],\n",
    "            [ 0.068120,  0.050119,  0.041998], [-0.078236,  0.048623,  0.003167],\n",
    "            # B1–B32 --------------------------------------------------\n",
    "            [ 0.095000,  0.047127,  0.000000], [ 0.088017,  0.044381, -0.029607],\n",
    "            [ 0.082092,  0.041635,  0.035895], [ 0.060069,  0.038888, -0.059242],\n",
    "            [ 0.044573,  0.036142,  0.066201], [ 0.015313,  0.033396, -0.079206],\n",
    "            [-0.015313,  0.033396,  0.079206], [-0.044573,  0.036142, -0.066201],\n",
    "            [-0.060069,  0.038888,  0.059242], [-0.082092,  0.041635, -0.035895],\n",
    "            [-0.088017,  0.044381,  0.029607], [-0.095000,  0.047127,  0.000000],\n",
    "            [-0.078236,  0.048623, -0.003167], [-0.068120,  0.050119, -0.041998],\n",
    "            [ 0.002514,  0.051615, -0.078259], [ 0.030532,  0.053111,  0.071740],\n",
    "            [-0.074536,  0.054607,  0.018668], [ 0.050861,  0.056103, -0.055513],\n",
    "            [-0.010572,  0.057599,  0.074345], [-0.032360,  0.059095, -0.062903],\n",
    "            [ 0.070070,  0.060591, -0.019264], [-0.055184,  0.062087,  0.040042],\n",
    "            [ 0.028852,  0.063583, -0.065405], [ 0.016635,  0.065079,  0.070415],\n",
    "            [-0.062635,  0.066575, -0.004402], [ 0.059341,  0.068071,  0.001459],\n",
    "            [-0.002143,  0.069567, -0.065043], [-0.048231,  0.071063, -0.036332],\n",
    "            [ 0.007880,  0.072559,  0.060812], [ 0.034223,  0.074055, -0.048679],\n",
    "            [-0.056250,  0.075551,  0.012366], [ 0.048085,  0.077047,  0.027866],\n",
    "            # C1–C32 --------------------------------------------------\n",
    "            [ 0.000000, -0.095000,  0.000000], [ 0.012383, -0.093504, -0.011344],\n",
    "            [-0.002068, -0.092008,  0.023564], [-0.017557, -0.090512, -0.022899],\n",
    "            [ 0.032677, -0.089016,  0.005780], [-0.031177, -0.087520,  0.019832],\n",
    "            [ 0.010465, -0.086024, -0.038928], [ 0.019985, -0.084528,  0.038480],\n",
    "            [-0.043359, -0.083031, -0.015835], [ 0.045066, -0.081535, -0.018602],\n",
    "            [-0.021690, -0.080039,  0.046349], [-0.015994, -0.078543, -0.050992],\n",
    "            [ 0.048085, -0.077047,  0.027866], [-0.056250, -0.075551,  0.012366],\n",
    "            [ 0.034223, -0.074055, -0.048679], [ 0.007880, -0.072559,  0.060812],\n",
    "            [-0.048231, -0.071063, -0.036332], [-0.002143, -0.069567,  0.065043],\n",
    "            [ 0.059341, -0.068071, -0.001459], [-0.062635, -0.066575, -0.004402],\n",
    "            [ 0.016635, -0.065079,  0.070415], [ 0.028852, -0.063583, -0.065405],\n",
    "            [-0.055184, -0.062087,  0.040042], [ 0.070070, -0.060591, -0.019264],\n",
    "            [-0.032360, -0.059095, -0.062903], [-0.010572, -0.057599,  0.074345],\n",
    "            [ 0.050861, -0.056103, -0.055513], [-0.074536, -0.054607,  0.018668],\n",
    "            [ 0.030532, -0.053111,  0.071740], [ 0.002514, -0.051615, -0.078259],\n",
    "            [-0.068120, -0.050119, -0.041998], [ 0.078236, -0.048623, -0.003167],\n",
    "            # D1–D32 --------------------------------------------------\n",
    "            [-0.095000, -0.047127,  0.000000], [-0.088017, -0.044381,  0.029607],\n",
    "            [-0.082092, -0.041635, -0.035895], [-0.060069, -0.038888,  0.059242],\n",
    "            [-0.044573, -0.036142, -0.066201], [-0.015313, -0.033396,  0.079206],\n",
    "            [ 0.015313, -0.033396, -0.079206], [ 0.044573, -0.036142,  0.066201],\n",
    "            [ 0.060069, -0.038888, -0.059242], [ 0.082092, -0.041635,  0.035895],\n",
    "            [ 0.088017, -0.044381, -0.029607], [ 0.095000, -0.047127,  0.000000],\n",
    "            [ 0.078236, -0.048623,  0.003167], [ 0.068120, -0.050119,  0.041998],\n",
    "            [-0.002514, -0.051615,  0.078259], [-0.030532, -0.053111, -0.071740],\n",
    "            [ 0.074536, -0.054607, -0.018668], [-0.050861, -0.056103,  0.055513],\n",
    "            [ 0.010572, -0.057599, -0.074345], [ 0.032360, -0.059095,  0.062903],\n",
    "            [-0.070070, -0.060591,  0.019264], [ 0.055184, -0.062087, -0.040042],\n",
    "            [-0.028852, -0.063583,  0.065405], [-0.016635, -0.065079, -0.070415],\n",
    "            [ 0.062635, -0.066575,  0.004402], [-0.059341, -0.068071, -0.001459],\n",
    "            [ 0.002143, -0.069567, -0.065043], [ 0.048231, -0.071063,  0.036332],\n",
    "            [-0.007880, -0.072559, -0.060812], [-0.034223, -0.074055,  0.048679],\n",
    "            [ 0.056250, -0.075551, -0.012366], [-0.048085, -0.077047, -0.027866]\n",
    "        ])\n",
    "        self.positions_3d = np.int_(self.positions_3d * 1000)\n",
    "\n",
    "        # -------- Channel name --------\n",
    "        self.channel_names   = np.array([f'{sec}{i+1}' for sec in \"ABCD\" for i in range(32)])\n",
    "        self.channel_to_index = {n: i for i, n in enumerate(self.channel_names)}\n",
    "        # -------- Learnable edge --------\n",
    "        self.edge_importance = nn.Parameter(\n",
    "            0.1 * torch.eye(Config.chan_num, device=Config.device) +\n",
    "            0.01 * torch.randn(Config.chan_num, Config.chan_num, device=Config.device),\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "    # -------- Basic distance weighted adjacency --------\n",
    "    def get_adjacency_matrix(self, calibration_constant=6.0, active_threshold=0.1):\n",
    "        dists = np.linalg.norm(self.positions_3d[:, None] - self.positions_3d, axis=-1)\n",
    "        with np.errstate(divide='ignore'):\n",
    "            weights = np.where(dists != 0, calibration_constant / dists, 0.0)\n",
    "        weights[weights < active_threshold] = 0.0\n",
    "        np.fill_diagonal(weights, 0.0)\n",
    "        if weights.max() > weights.min():\n",
    "            weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "        np.fill_diagonal(weights, 1.0)\n",
    "        return weights.astype(np.float32)\n",
    "\n",
    "# ---------- Residual GCN ----------\n",
    "class resGCN(nn.Module):\n",
    "    def __init__(self, dim, band):\n",
    "        super().__init__()\n",
    "        inc  = dim * band\n",
    "        outc = Config.chan_num\n",
    "        self.g1 = nn.Conv2d(inc,  outc, (1,3), padding=(0,1), bias=False)\n",
    "        self.g2 = nn.Conv2d(outc, outc, (1,1), bias=False)\n",
    "        self.bn1, self.bn2 = nn.BatchNorm2d(outc), nn.BatchNorm2d(outc)\n",
    "        self.elu  = nn.ELU(False)\n",
    "    def forward(self,x,x_p,L):\n",
    "        return self.elu(self.bn2(self.g2(self.elu(self.bn1(self.g1(x))))))\n",
    "\n",
    "# ---------- HGCN ----------\n",
    "class HGCN_Dual(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, tau_mm: float = 25.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.elec = Electrodes()\n",
    "        self.tau  = tau_mm\n",
    "        # Two independent residuals GCN\n",
    "        self.gcn_short = resGCN(dim, Config.band_num)\n",
    "        self.gcn_long  = resGCN(dim, Config.band_num)\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize(A: torch.Tensor):\n",
    "        A = A + torch.eye(A.shape[-1], device=A.device)\n",
    "        deg = A.sum(-1, keepdim=True)\n",
    "        D_inv_sqrt = deg.pow(-0.5)\n",
    "        D_inv_sqrt[torch.isinf(D_inv_sqrt)] = 0.0\n",
    "        return D_inv_sqrt * A * D_inv_sqrt.transpose(-1, -2)\n",
    "\n",
    "\n",
    "    def _split_adj(self, base_adj: torch.Tensor):\n",
    "\n",
    "        pos = torch.tensor(self.elec.positions_3d,\n",
    "                           dtype=torch.float32, device=base_adj.device)\n",
    "        dist = torch.cdist(pos, pos)          # [128,128] (mm)\n",
    "        mask_short = (dist < self.tau).float()\n",
    "        mask_long  = (dist >= self.tau).float()\n",
    "        A_short = self._normalize(base_adj * mask_short)\n",
    "        A_long  = self._normalize(base_adj * mask_long)\n",
    "        return A_short, A_long\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        base_adj = torch.tensor(self.elec.get_adjacency_matrix(),\n",
    "                                dtype=torch.float32, device=x.device)\n",
    "        # Add a learning edge\n",
    "        base_adj = base_adj + self.elec.edge_importance\n",
    "        base_adj = (base_adj + base_adj.t()) / 2.0\n",
    "\n",
    "        A_short, A_long = self._split_adj(base_adj)\n",
    "\n",
    "        out_s = self.gcn_short(x, x, A_short)\n",
    "        out_l = self.gcn_long (x, x, A_long)\n",
    "        return out_s + out_l   \n",
    "\n",
    "\n",
    "class SGCN_Dual(nn.Module):\n",
    "    def __init__(self, dim, tau_mm: float = 25.0):\n",
    "        super().__init__()\n",
    "        self.hgcn = HGCN_Dual(dim, tau_mm)\n",
    "        self.elec = self.hgcn.elec\n",
    "    def forward(self,x):\n",
    "        return self.hgcn(x)\n",
    "\n",
    "# ============================================================\n",
    "#                     EEGEncoder\n",
    "# ============================================================\n",
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # --- Temporal ---\n",
    "        self.t_block  = Temporal_Block()\n",
    "        self.tgcn     = SGCN_Dual(dim=5)                       \n",
    "        self.chanattn = nn.Conv2d(Config.chan_num, Config.inc, 1)  # 128→32\n",
    "        self.proj1    = nn.Linear(256 * Config.chan_num, 256)\n",
    "\n",
    "        # --- Spectral ---\n",
    "        self.dgcn = SGCN_Dual(dim=1)                         \n",
    "        self.pgcn = SGCN_Dual(dim=1)                           \n",
    "        self.mlp_low  = nn.Sequential(nn.Linear(128,128), nn.ELU(False),\n",
    "                                      nn.Linear(128,128))\n",
    "        self.mlp_high = nn.Sequential(nn.Linear(128,128), nn.ELU(False),\n",
    "                                      nn.Linear(128,128))\n",
    "        self.spec_up  = nn.ConvTranspose1d(128,128,4,4)\n",
    "\n",
    "        # --- Fuse ---\n",
    "        self.final_conv = nn.Conv2d(288,128,1)\n",
    "        self.dpout      = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, filtered, prefeat):\n",
    "        B = filtered.size(0)\n",
    "\n",
    "        # ----- Temporal -----\n",
    "        x = filtered.unsqueeze(1).expand(-1,5,-1,-1)           # [B,5,128,256]\n",
    "        t = self.t_block(x)                                    # [B,25,128,256]\n",
    "        t = self.tgcn(t)                                       # [B,128,128,256]\n",
    "        t = self.chanattn(t)                                   # [B,32,128,256]\n",
    "        t = t.permute(0,3,1,2).contiguous()                    # [B,256,32,128]\n",
    "        t = t.reshape(B, Config.inc, -1)                       # [B,32,32768]\n",
    "        t = self.proj1(t)                                      # [B,32,256]\n",
    "\n",
    "        # ----- Spectral -----\n",
    "        low  = self.dgcn(prefeat[:,:, :5].permute(0,2,1).unsqueeze(3)).squeeze(-1)\n",
    "        high = self.pgcn(prefeat[:,:,5: ].permute(0,2,1).unsqueeze(3)).squeeze(-1)\n",
    "        low  = self.spec_up(self.mlp_low (low ))[:, :, :256]\n",
    "        high = self.spec_up(self.mlp_high(high))[:, :, :256]\n",
    "        spec = torch.cat((low, high), 1)                       # [B,256,256]\n",
    "\n",
    "        # ----- Fuse & Output -----\n",
    "        fused = torch.cat((t, spec), 1).unsqueeze(-1)          # [B,288,256,1]\n",
    "        out   = self.final_conv(fused).squeeze(-1)             # [B,128,256]\n",
    "        return self.dpout(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40747258",
   "metadata": {},
   "source": [
    "## AudioEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cc942af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:36.290302Z",
     "start_time": "2025-08-09T03:24:36.141066Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, L, N):\n",
    "        super(AudioEncoder, self).__init__()\n",
    "        self.L, self.N = L, N\n",
    "        self.conv1d_U = nn.Conv1d(1, N, kernel_size=L, stride=L // 2, bias=False)\n",
    "\n",
    "    def forward(self, mixture):\n",
    "        \"\"\"\n",
    "            mixture: [M, T], M is batch size, T is #samples\n",
    "        Returns:\n",
    "            mixture_w: [M, N, K], where K = (T-L)/(L/2)+1 = 2T/L-1\n",
    "        \"\"\"\n",
    "        #print(\"----1----\",mixture.shape)\n",
    "        mixture_w = F.relu(self.conv1d_U(mixture.unsqueeze(1)))  # [M, N, K]().unsqueeze(0)\n",
    "        return mixture_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416ad02",
   "metadata": {},
   "source": [
    "## utils_mossformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aac5ba",
   "metadata": {},
   "source": [
    "### RotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb536c-2291-47c5-a72e-0d970ac643f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(x): return x is not None\n",
    "\n",
    "def rotate_half(x: Tensor) -> Tensor:\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "@torch.cuda.amp.autocast(enabled=False)\n",
    "def _apply_rotary(\n",
    "    angles: Tensor,\n",
    "    t: Tensor,\n",
    "    *,\n",
    "    start_index: int = 0,\n",
    "    scale: Tensor | float = 1.0,\n",
    "    seq_dim: int = -2,\n",
    ") -> Tensor:\n",
    "    if t.ndim == 3:                        \n",
    "        angles = angles[-t.shape[seq_dim]:]\n",
    "\n",
    "    rot_d = angles.shape[-1]\n",
    "    end_idx = start_index + rot_d\n",
    "    assert rot_d <= t.shape[-1], (\n",
    "        f\"rotation dim {rot_d} > tensor dim {t.shape[-1]}\"\n",
    "    )\n",
    "\n",
    "    left, mid, right = t[..., :start_index], t[..., start_index:end_idx], t[..., end_idx:]\n",
    "    mid = (mid * angles.cos() * scale) + (rotate_half(mid) * angles.sin() * scale)\n",
    "    return torch.cat((left, mid, right), dim=-1).type_as(t)\n",
    "\n",
    "# ---------------- RotaryEmbedding ---------------- #\n",
    "class RotaryEmbedding(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        *,\n",
    "        freqs_for: Literal[\"lang\", \"pixel\", \"constant\"] = \"lang\",\n",
    "        theta: float = 10000.0,\n",
    "        max_freq: int = 10,\n",
    "        num_freqs: int = 1,\n",
    "        learned_freq: bool = False,\n",
    "        use_xpos: bool = False,\n",
    "        xpos_scale_base: int = 512,\n",
    "        interpolate_factor: float = 1.0,\n",
    "        theta_rescale_factor: float = 1.0,\n",
    "        seq_before_head_dim: bool = False,\n",
    "        cache_if_possible: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # —— frequency —— #\n",
    "        theta *= theta_rescale_factor ** (dim / max(dim - 2, 1))\n",
    "        if freqs_for == \"lang\":\n",
    "            freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        elif freqs_for == \"pixel\":\n",
    "            freqs = torch.linspace(1.0, max_freq / 2, dim // 2) * pi\n",
    "        else:  # 'constant'\n",
    "            freqs = torch.ones(num_freqs)\n",
    "\n",
    "        self.freqs = torch.nn.Parameter(freqs, requires_grad=learned_freq)\n",
    "        self.freqs_for = freqs_for\n",
    "        self.learned_freq = learned_freq\n",
    "\n",
    "        self.seq_before_head_dim = seq_before_head_dim\n",
    "        self.default_seq_dim = -3 if seq_before_head_dim else -2\n",
    "\n",
    "        self.interpolate_factor = max(1.0, interpolate_factor)\n",
    "        self.use_xpos = use_xpos\n",
    "        self.xpos_scale_base = xpos_scale_base\n",
    "        self.cache_if_possible = cache_if_possible\n",
    "        self._angle_cache: Dict[Tuple[int, torch.device, torch.dtype], Tensor] = {}\n",
    "        self._scale_cache: Dict[Tuple[int, torch.device, torch.dtype], Tensor] = {}\n",
    "\n",
    "        if use_xpos:\n",
    "            scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "            self.register_buffer(\"xpos_scale\", scale)\n",
    "        else:\n",
    "            self.register_buffer(\"xpos_scale\", torch.tensor([]))  \n",
    "\n",
    "    def _get_angles(\n",
    "        self, seq_len: int, *, device: torch.device, dtype: torch.dtype\n",
    "    ) -> Tensor:\n",
    "        if self.interpolate_factor != 1.0 and self.freqs_for == \"lang\":\n",
    "            pos = torch.arange(seq_len, device=device, dtype=dtype)\n",
    "            pos = pos / self.interpolate_factor\n",
    "            angles = einsum(\"i , j -> i j\", pos, self.freqs.to(device=device, dtype=dtype))\n",
    "        else:\n",
    "            angles = einsum(\n",
    "                \"i , j -> i j\",\n",
    "                torch.arange(seq_len, device=device, dtype=dtype),\n",
    "                self.freqs.to(device=device, dtype=dtype),\n",
    "            )\n",
    "        return torch.cat((angles, angles), dim=-1)  \n",
    "\n",
    "    def _maybe_get_cached(\n",
    "        self, seq_len: int, *, device: torch.device, dtype: torch.dtype\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        key = (seq_len, device, dtype)\n",
    "        if not self.cache_if_possible or key not in self._angle_cache:\n",
    "            angles = self._get_angles(seq_len, device=device, dtype=dtype)\n",
    "            self._angle_cache[key] = angles  \n",
    "            if self.use_xpos:\n",
    "                s = (\n",
    "                    (torch.arange(seq_len, device=device, dtype=dtype) + 0.5)\n",
    "                    / self.xpos_scale_base\n",
    "                ) ** self.xpos_scale\n",
    "                self._scale_cache[key] = torch.cat((s, s), dim=-1)\n",
    "        return self._angle_cache[key], (\n",
    "            self._scale_cache[key] if self.use_xpos else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, seq_len: int, *, device: torch.device | None = None, dtype: torch.dtype | None = None\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        device = device or self.freqs.device\n",
    "        dtype = dtype or self.freqs.dtype\n",
    "        angles, _ = self._maybe_get_cached(seq_len, device=device, dtype=dtype)\n",
    "        return angles.sin(), angles.cos()\n",
    "\n",
    "    def rotate_queries_or_keys(\n",
    "        self,\n",
    "        t: Tensor,\n",
    "        *,\n",
    "        seq_dim: int | None = None,\n",
    "        offset: int = 0,\n",
    "        start_index: int = 0,\n",
    "    ) -> Tensor:\n",
    "        seq_dim = self.default_seq_dim if seq_dim is None else seq_dim\n",
    "        seq_len = t.shape[seq_dim]\n",
    "        angles, scale = self._maybe_get_cached(seq_len + offset, device=t.device, dtype=t.dtype)\n",
    "        angles = angles[offset : offset + seq_len]\n",
    "        if isinstance(scale, Tensor):\n",
    "            scale = scale[offset : offset + seq_len]\n",
    "\n",
    "        while angles.ndim < t.ndim:\n",
    "            angles = angles.unsqueeze(0)\n",
    "        if isinstance(scale, Tensor) and scale.ndim < t.ndim:\n",
    "            scale = scale.unsqueeze(0)\n",
    "\n",
    "        return _apply_rotary(angles, t, start_index=start_index, scale=scale, seq_dim=seq_dim)\n",
    "\n",
    "    def rotate_queries_with_cached_keys(\n",
    "        self,\n",
    "        q: Tensor,\n",
    "        k: Tensor,\n",
    "        *,\n",
    "        seq_dim: int | None = None,\n",
    "        start_index: int = 0,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        seq_dim = self.default_seq_dim if seq_dim is None else seq_dim\n",
    "        offset = k.shape[seq_dim] - q.shape[seq_dim]\n",
    "        q = self.rotate_queries_or_keys(q, seq_dim=seq_dim, offset=offset, start_index=start_index)\n",
    "        k = self.rotate_queries_or_keys(k, seq_dim=seq_dim, start_index=start_index)\n",
    "        return q, k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71747fa2",
   "metadata": {},
   "source": [
    "### fsmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e87af3a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:36.761343Z",
     "start_time": "2025-08-09T03:24:36.593248Z"
    }
   },
   "outputs": [],
   "source": [
    "class UniDeepFsmn(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, lorder=None, hidden_size=None):\n",
    "        super(UniDeepFsmn, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if lorder is None:\n",
    "            return\n",
    "\n",
    "        self.lorder = lorder\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, hidden_size)\n",
    "\n",
    "        self.project = nn.Linear(hidden_size, output_dim, bias=False)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(output_dim, output_dim, [lorder+lorder-1, 1], [1, 1], groups=output_dim, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        f1 = F.relu(self.linear(input))\n",
    "\n",
    "        p1 = self.project(f1)\n",
    "\n",
    "        x = torch.unsqueeze(p1, 1)\n",
    "\n",
    "        x_per = x.permute(0, 3, 2, 1)\n",
    "\n",
    "        y = F.pad(x_per, [0, 0, self.lorder - 1, self.lorder - 1])\n",
    "\n",
    "        out = x_per + self.conv1(y)\n",
    "\n",
    "        out1 = out.permute(0, 3, 2, 1)\n",
    "\n",
    "        return input + out1.squeeze()\n",
    "\n",
    "class DilatedDenseNet(nn.Module):\n",
    "    def __init__(self, depth=4, lorder=20, in_channels=64):\n",
    "        super(DilatedDenseNet, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.in_channels = in_channels\n",
    "        self.pad = nn.ConstantPad2d((1, 1, 1, 0), value=0.)\n",
    "        self.twidth = lorder*2-1\n",
    "        self.kernel_size = (self.twidth, 1)\n",
    "        for i in range(self.depth):\n",
    "            dil = 2 ** i\n",
    "            pad_length = lorder + (dil - 1) * (lorder - 1) - 1\n",
    "            setattr(self, 'pad{}'.format(i + 1), nn.ConstantPad2d((0, 0, pad_length, pad_length), value=0.))\n",
    "            setattr(self, 'conv{}'.format(i + 1),\n",
    "                    nn.Conv2d(self.in_channels*(i+1), self.in_channels, kernel_size=self.kernel_size,\n",
    "                              dilation=(dil, 1), groups=self.in_channels, bias=False))\n",
    "            setattr(self, 'norm{}'.format(i + 1), nn.InstanceNorm2d(in_channels, affine=True))\n",
    "            setattr(self, 'prelu{}'.format(i + 1), nn.PReLU(self.in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x\n",
    "        for i in range(self.depth):\n",
    "            out = getattr(self, 'pad{}'.format(i + 1))(skip)\n",
    "            out = getattr(self, 'conv{}'.format(i + 1))(out)\n",
    "            out = getattr(self, 'norm{}'.format(i + 1))(out)\n",
    "            out = getattr(self, 'prelu{}'.format(i + 1))(out)            \n",
    "            skip = torch.cat([out, skip], dim=1)\n",
    "        return out\n",
    "\n",
    "class UniDeepFsmn_dilated(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, lorder=None, hidden_size=None):\n",
    "        super(UniDeepFsmn_dilated, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if lorder is None:\n",
    "            return\n",
    "\n",
    "        self.lorder = lorder\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, hidden_size)\n",
    "\n",
    "        self.project = nn.Linear(hidden_size, output_dim, bias=False)\n",
    "\n",
    "        self.conv = DilatedDenseNet(depth=2, lorder=lorder, in_channels=output_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        f1 = F.relu(self.linear(input))\n",
    "\n",
    "        p1 = self.project(f1)\n",
    "\n",
    "        x = torch.unsqueeze(p1, 1)\n",
    "\n",
    "        x_per = x.permute(0, 3, 2, 1)\n",
    "\n",
    "        out = self.conv(x_per)\n",
    "\n",
    "        out1 = out.permute(0, 3, 2, 1)\n",
    "\n",
    "        return input + out1.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c3dcc",
   "metadata": {},
   "source": [
    "### normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb66212d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:36.921576Z",
     "start_time": "2025-08-09T03:24:36.768680Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    This code came from sb.nnet.normalization\n",
    "    # from sb.nnet.normalization import LayerNorm\n",
    "\n",
    "\n",
    "    Applies layer normalization to the input tensor.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    input_shape : tuple\n",
    "        The expected shape of the input.\n",
    "    eps : float\n",
    "        This value is added to std deviation estimation to improve the numerical\n",
    "        stability.\n",
    "    elementwise_affine : bool\n",
    "        If True, this module has learnable per-element affine parameters\n",
    "        initialized to ones (for weights) and zeros (for biases).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> input = torch.randn(100, 101, 128)\n",
    "    >>> norm = LayerNorm(input_shape=input.shape)\n",
    "    >>> output = norm(input)\n",
    "    >>> output.shape\n",
    "    torch.Size([100, 101, 128])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=None,\n",
    "        input_shape=None,\n",
    "        eps=1e-05,\n",
    "        elementwise_affine=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        if input_shape is not None:\n",
    "            input_size = input_shape[2:]\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(\n",
    "            input_size,\n",
    "            eps=self.eps,\n",
    "            elementwise_affine=self.elementwise_affine,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns the normalized input tensor.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor (batch, time, channels)\n",
    "            input to normalize. 3d or 4d tensors are expected.\n",
    "        \"\"\"\n",
    "        return self.norm(x)\n",
    "\n",
    "class CLayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Channel-wise layer normalization.\"\"\"\n",
    "\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super(CLayerNorm, self).__init__(*args,**kwargs)\n",
    "\n",
    "    def forward(self, sample):\n",
    "        \"\"\"Forward function.\n",
    "            sample: [batch_size, channels, length]\n",
    "        \"\"\"\n",
    "        if sample.dim() != 3:\n",
    "            raise RuntimeError('{} only accept 3-D tensor as input'.format(\n",
    "                self.__name__))\n",
    "        # [N, C, T] -> [N, T, C]\n",
    "        sample = torch.transpose(sample, 1, 2)\n",
    "        # LayerNorm\n",
    "        sample = super().forward(sample)\n",
    "        # [N, T, C] -> [N, C, T]\n",
    "        sample = torch.transpose(sample, 1, 2)\n",
    "        return sample\n",
    "\n",
    "class ScaleNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.norm(x, dim = -1, keepdim = True) * self.scale\n",
    "        return x / norm.clamp(min = self.eps) * self.g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b2f70",
   "metadata": {},
   "source": [
    "### conv_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1e9e302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:37.137172Z",
     "start_time": "2025-08-09T03:24:36.923216Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transpose(nn.Module):\n",
    "    \"\"\" Wrapper class of torch.transpose() for Sequential module. \"\"\"\n",
    "    def __init__(self, shape: tuple):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.transpose(*self.shape)\n",
    "\n",
    "class DepthwiseConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    When groups == in_channels and out_channels == K * in_channels, where K is a positive integer,\n",
    "    this operation is termed in literature as depthwise convolution.\n",
    "    \n",
    "        in_channels (int): Number of channels in the input\n",
    "        out_channels (int): Number of channels produced by the convolution\n",
    "        kernel_size (int or tuple): Size of the convolving kernel\n",
    "        stride (int, optional): Stride of the convolution. Default: 1\n",
    "        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
    "        bias (bool, optional): If True, adds a learnable bias to the output. Default: True\n",
    "    Inputs: inputs\n",
    "        - **inputs** (batch, in_channels, time): Tensor containing input vector\n",
    "    Returns: outputs\n",
    "        - **outputs** (batch, out_channels, time): Tensor produces by depthwise 1-D convolution.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            kernel_size: int,\n",
    "            stride: int = 1,\n",
    "            padding: int = 0,\n",
    "            bias: bool = False,\n",
    "    ) -> None:\n",
    "        super(DepthwiseConv1d, self).__init__()\n",
    "        assert out_channels % in_channels == 0, \"out_channels should be constant multiple of in_channels\"\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            groups=in_channels,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        return self.conv(inputs)\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Conformer convolution module starts with a pointwise convolution and a gated linear unit (GLU).\n",
    "    This is followed by a single 1-D depthwise convolution layer. Batchnorm is  deployed just after the convolution\n",
    "    to aid training deep models.\n",
    "        in_channels (int): Number of channels in the input\n",
    "        kernel_size (int or tuple, optional): Size of the convolving kernel Default: 31\n",
    "        dropout_p (float, optional): probability of dropout\n",
    "    Inputs: inputs\n",
    "        inputs (batch, time, dim): Tensor contains input sequences\n",
    "    Outputs: outputs\n",
    "        outputs (batch, time, dim): Tensor produces by conformer convolution module.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            kernel_size: int = 17,\n",
    "            expansion_factor: int = 2,\n",
    "            dropout_p: float = 0.1,\n",
    "    ) -> None:\n",
    "        super(ConvModule, self).__init__()\n",
    "        assert (kernel_size - 1) % 2 == 0, \"kernel_size should be a odd number for 'SAME' padding\"\n",
    "        assert expansion_factor == 2, \"Currently, Only Supports expansion_factor 2\"\n",
    "\n",
    "        self.sequential = nn.Sequential(\n",
    "            Transpose(shape=(1, 2)),\n",
    "            DepthwiseConv1d(in_channels, in_channels, kernel_size, stride=1, padding=(kernel_size - 1) // 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tensor:\n",
    "        return inputs + self.sequential(inputs).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab159ebb",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c406cda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:37.325732Z",
     "start_time": "2025-08-09T03:24:37.139010Z"
    }
   },
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def padding_to_multiple_of(n, mult):\n",
    "    remainder = n % mult\n",
    "    if remainder == 0:\n",
    "        return 0\n",
    "    return mult - remainder\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "class FFConvM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_out,\n",
    "        norm_klass = nn.LayerNorm,\n",
    "        dropout = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mdl = nn.Sequential(\n",
    "            norm_klass(dim_in),\n",
    "            nn.Linear(dim_in, dim_out),\n",
    "            nn.SiLU(),\n",
    "            ConvModule(dim_out),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        output = self.mdl(x)\n",
    "        return output\n",
    "\n",
    "class Gated_FSMN_dilated(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        lorder,\n",
    "        hidden_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.to_u = FFConvM(\n",
    "            dim_in = in_channels,\n",
    "            dim_out = hidden_size,\n",
    "            norm_klass = nn.LayerNorm,\n",
    "            dropout = 0.1,\n",
    "            )\n",
    "        self.to_v = FFConvM(\n",
    "            dim_in = in_channels,\n",
    "            dim_out = hidden_size,\n",
    "            norm_klass = nn.LayerNorm,\n",
    "            dropout = 0.1,\n",
    "            )\n",
    "        self.fsmn = UniDeepFsmn_dilated(in_channels, out_channels, lorder, hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        input = x\n",
    "        x_u = self.to_u(x)\n",
    "        x_v = self.to_v(x) \n",
    "        x_u = self.fsmn(x_u)\n",
    "        x = x_v * x_u + input               \n",
    "        return x\n",
    "\n",
    "class Gated_FSMN_Block_Dilated(nn.Module):\n",
    "    \"\"\"1-D convolutional block.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 inner_channels = 256,\n",
    "                 group_size = 256, #384, #128, #256,\n",
    "                 #query_key_dim = 128, #256, #128,\n",
    "                 #expansion_factor = 4.,\n",
    "                 #causal = False,\n",
    "                 #dropout = 0.1,\n",
    "                 norm_type = 'scalenorm',\n",
    "                 #shift_tokens = True,\n",
    "                 #rotary_pos_emb = None,\n",
    "                 ):\n",
    "        super(Gated_FSMN_Block_Dilated, self).__init__()\n",
    "        if norm_type == 'scalenorm':\n",
    "            norm_klass = ScaleNorm\n",
    "        elif norm_type == 'layernorm':\n",
    "            norm_klass = nn.LayerNorm\n",
    "\n",
    "        self.group_size = group_size\n",
    "\n",
    "        # rotary_pos_emb = RotaryEmbedding(dim = min(32, query_key_dim))\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(dim, inner_channels, kernel_size=1),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        self.norm1 = CLayerNorm(inner_channels)\n",
    "        #block dilated without gating\n",
    "        #self.gated_fsmn = UniDeepFsmn_dilated(inner_channels, inner_channels, 20, inner_channels)\n",
    "        #block dilated with gating\n",
    "        self.gated_fsmn = Gated_FSMN_dilated(inner_channels, inner_channels, lorder=20, hidden_size=inner_channels)\n",
    "        self.norm2 = CLayerNorm(inner_channels)\n",
    "        self.conv2 = nn.Conv1d(inner_channels, dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        conv1 = self.conv1(input.transpose(2,1))\n",
    "        norm1 = self.norm1(conv1)\n",
    "        seq_out = self.gated_fsmn(norm1.transpose(2,1))\n",
    "        norm2 = self.norm2(seq_out.transpose(2,1))\n",
    "        conv2 = self.conv2(norm2)\n",
    "        return conv2.transpose(2,1) + input\n",
    "\n",
    "class OffsetScale(nn.Module):\n",
    "    def __init__(self, dim, heads = 1):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(heads, dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(heads, dim))\n",
    "        nn.init.normal_(self.gamma, std = 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = einsum('... d, h d -> ... h d', x, self.gamma) + self.beta\n",
    "        return out.unbind(dim = -2)\n",
    "        \n",
    "class FLASH_ShareA_FFConvM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        group_size = 256,\n",
    "        query_key_dim = 128,\n",
    "        expansion_factor = 1.,\n",
    "        causal = False,\n",
    "        dropout = 0.1,\n",
    "        rotary_pos_emb = None,\n",
    "        norm_klass = nn.LayerNorm,\n",
    "        shift_tokens = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(dim * expansion_factor)        \n",
    "        self.group_size = group_size\n",
    "        self.causal = causal\n",
    "        self.shift_tokens = shift_tokens\n",
    "\n",
    "        # positional embeddings\n",
    "        self.rotary_pos_emb = rotary_pos_emb\n",
    "        # norm\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #self.move = MultiHeadEMA(embed_dim=dim, ndim=4, bidirectional=False, truncation=None)\n",
    "        # projections\n",
    "        \n",
    "        self.to_hidden = FFConvM(\n",
    "            dim_in = dim,\n",
    "            dim_out = hidden_dim,\n",
    "            norm_klass = norm_klass,\n",
    "            dropout = dropout,\n",
    "            )\n",
    "        self.to_qk = FFConvM(\n",
    "            dim_in = dim,\n",
    "            dim_out = query_key_dim,\n",
    "            norm_klass = norm_klass,\n",
    "            dropout = dropout,\n",
    "            )\n",
    "\n",
    "        self.qk_offset_scale = OffsetScale(query_key_dim, heads = 4)\n",
    "\n",
    "        self.to_out = FFConvM(\n",
    "            dim_in = dim*2,\n",
    "            dim_out = dim,\n",
    "            norm_klass = norm_klass,\n",
    "            dropout = dropout,\n",
    "            )\n",
    "        \n",
    "        self.gateActivate=nn.Sigmoid() #exp3\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        *,\n",
    "        mask = None\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        b - batch\n",
    "        n - sequence length (within groups)\n",
    "        g - group dimension\n",
    "        d - feature dimension (keys)\n",
    "        e - feature dimension (values)\n",
    "        i - sequence dimension (source)\n",
    "        j - sequence dimension (target)\n",
    "        \"\"\"\n",
    "\n",
    "        #b, n, device, g = x.shape[0], x.shape[-2], x.device, self.group_size\n",
    "\n",
    "        # prenorm\n",
    "        #x = self.fsmn(x)\n",
    "        normed_x = x #self.norm(x)\n",
    "\n",
    "        # do token shift - a great, costless trick from an independent AI researcher in Shenzhen\n",
    "        residual = x\n",
    "\n",
    "        if self.shift_tokens:\n",
    "            x_shift, x_pass = normed_x.chunk(2, dim = -1)\n",
    "            x_shift = F.pad(x_shift, (0, 0, 1, -1), value = 0.)\n",
    "            normed_x = torch.cat((x_shift, x_pass), dim = -1)\n",
    "\n",
    "        # initial projections\n",
    "\n",
    "        v, u = self.to_hidden(normed_x).chunk(2, dim = -1)\n",
    "        qk = self.to_qk(normed_x)\n",
    "        #print('normed_x: {}'.format(normed_x.shape)) \n",
    "\n",
    "        # offset and scale\n",
    "        quad_q, lin_q, quad_k, lin_k = self.qk_offset_scale(qk)\n",
    "        #print('q {}, k {}, v {}'.format(quad_q.shape, quad_k.shape, v.shape))\n",
    "        att_v, att_u = self.cal_attention(x, quad_q, lin_q, quad_k, lin_k, v, u)\n",
    "\n",
    "        #exp5: self.gateActivate=nn.SiLU()\n",
    "        out = (att_u*v ) * self.gateActivate(att_v*u)\n",
    "        \n",
    "        x = x + self.to_out(out)\n",
    "        #x = x + self.conv_module(x)\n",
    "        return x\n",
    "\n",
    "    def cal_attention(self, x, quad_q, lin_q, quad_k, lin_k, v, u, mask = None):\n",
    "        b, n, device, g = x.shape[0], x.shape[-2], x.device, self.group_size\n",
    "\n",
    "        if exists(mask):\n",
    "            lin_mask = rearrange(mask, '... -> ... 1')\n",
    "            lin_k = lin_k.masked_fill(~lin_mask, 0.)\n",
    "\n",
    "        # rotate queries and keys\n",
    "\n",
    "        if exists(self.rotary_pos_emb):\n",
    "            quad_q, lin_q, quad_k, lin_k = map(self.rotary_pos_emb.rotate_queries_or_keys, (quad_q, lin_q, quad_k, lin_k))\n",
    "\n",
    "        # padding for groups\n",
    "\n",
    "        padding = padding_to_multiple_of(n, g)\n",
    "\n",
    "        if padding > 0:\n",
    "            quad_q, quad_k, lin_q, lin_k, v, u = map(lambda t: F.pad(t, (0, 0, 0, padding), value = 0.), (quad_q, quad_k, lin_q, lin_k, v, u))\n",
    "\n",
    "            mask = default(mask, torch.ones((b, n), device = device, dtype = torch.bool))\n",
    "            mask = F.pad(mask, (0, padding), value = False)\n",
    "\n",
    "        # group along sequence\n",
    "\n",
    "        quad_q, quad_k, lin_q, lin_k, v, u = map(lambda t: rearrange(t, 'b (g n) d -> b g n d', n = self.group_size), (quad_q, quad_k, lin_q, lin_k, v, u))\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b (g j) -> b g 1 j', j = g)\n",
    "\n",
    "        # calculate quadratic attention output\n",
    "\n",
    "        sim = einsum('... i d, ... j d -> ... i j', quad_q, quad_k) / g\n",
    "\n",
    "        ###eddy REMOVE this part can solve infinite loss prob!!!!!!!!!!!!!\n",
    "        #sim = sim + self.rel_pos_bias(sim)\n",
    "\n",
    "        attn = F.relu(sim) ** 2\n",
    "        #attn = F.relu(sim)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        if exists(mask):\n",
    "            attn = attn.masked_fill(~mask, 0.)\n",
    "\n",
    "        if self.causal:\n",
    "            causal_mask = torch.ones((g, g), dtype = torch.bool, device = device).triu(1)\n",
    "            attn = attn.masked_fill(causal_mask, 0.)\n",
    "\n",
    "        quad_out_v = einsum('... i j, ... j d -> ... i d', attn, v)\n",
    "        quad_out_u = einsum('... i j, ... j d -> ... i d', attn, u)\n",
    "\n",
    "        # calculate linear attention output\n",
    "\n",
    "        if self.causal:\n",
    "            lin_kv = einsum('b g n d, b g n e -> b g d e', lin_k, v) / g\n",
    "            # exclusive cumulative sum along group dimension\n",
    "            lin_kv = lin_kv.cumsum(dim = 1)\n",
    "            lin_kv = F.pad(lin_kv, (0, 0, 0, 0, 1, -1), value = 0.)\n",
    "            lin_out_v = einsum('b g d e, b g n d -> b g n e', lin_kv, lin_q)\n",
    "\n",
    "            lin_ku = einsum('b g n d, b g n e -> b g d e', lin_k, u) / g\n",
    "            # exclusive cumulative sum along group dimension\n",
    "            lin_ku = lin_ku.cumsum(dim = 1)\n",
    "            lin_ku = F.pad(lin_ku, (0, 0, 0, 0, 1, -1), value = 0.)\n",
    "            lin_out_u = einsum('b g d e, b g n d -> b g n e', lin_ku, lin_q)\n",
    "        else:\n",
    "            lin_kv = einsum('b g n d, b g n e -> b d e', lin_k, v) / n\n",
    "            lin_out_v = einsum('b g n d, b d e -> b g n e', lin_q, lin_kv)\n",
    "\n",
    "            lin_ku = einsum('b g n d, b g n e -> b d e', lin_k, u) / n\n",
    "            lin_out_u = einsum('b g n d, b d e -> b g n e', lin_q, lin_ku)\n",
    "\n",
    "        # fold back groups into full sequence, and excise out padding\n",
    "        '''\n",
    "        quad_attn_out_v, lin_attn_out_v = map(lambda t: rearrange(t, 'b g n d -> b (g n) d')[:, :n], (quad_out_v, lin_out_v))\n",
    "        quad_attn_out_u, lin_attn_out_u = map(lambda t: rearrange(t, 'b g n d -> b (g n) d')[:, :n], (quad_out_u, lin_out_u))\n",
    "        return quad_attn_out_v+lin_attn_out_v, quad_attn_out_u+lin_attn_out_u\n",
    "        '''\n",
    "        return map(lambda t: rearrange(t, 'b g n d -> b (g n) d')[:, :n], (quad_out_v+lin_out_v, quad_out_u+lin_out_u))\n",
    "\n",
    "class FLASHTransformer_DualA_FSMN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth,\n",
    "        group_size = 256, #384, #128, #256,\n",
    "        query_key_dim = 128, #256, #128,\n",
    "        expansion_factor = 4.,\n",
    "        causal = False,\n",
    "        attn_dropout = 0.1,\n",
    "        norm_type = 'scalenorm',\n",
    "        shift_tokens = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert norm_type in ('scalenorm', 'layernorm'), 'norm_type must be one of scalenorm or layernorm'\n",
    "\n",
    "        if norm_type == 'scalenorm':\n",
    "            norm_klass = ScaleNorm\n",
    "        elif norm_type == 'layernorm':\n",
    "            norm_klass = nn.LayerNorm\n",
    "\n",
    "        self.group_size = group_size\n",
    "\n",
    "        rotary_pos_emb = RotaryEmbedding(dim = min(32, query_key_dim))\n",
    "        # max rotary embedding dimensions of 32, partial Rotary embeddings, from Wang et al - GPT-J\n",
    "        #self.fsmn = nn.ModuleList([Gated_FSMN(dim, dim, lorder=20, hidden_size=dim) for _ in range(depth)])\n",
    "        #self.fsmn = nn.ModuleList([Gated_FSMN_Block(dim) for _ in range(depth)])\n",
    "        self.fsmn = nn.ModuleList([Gated_FSMN_Block_Dilated(dim) for _ in range(depth)])\n",
    "        self.layers = nn.ModuleList([FLASH_ShareA_FFConvM(dim = dim, group_size = group_size, query_key_dim = query_key_dim, expansion_factor = expansion_factor, causal = causal, dropout = attn_dropout, rotary_pos_emb = rotary_pos_emb, norm_klass = norm_klass, shift_tokens = shift_tokens) for _ in range(depth)])\n",
    "\n",
    "    def _build_repeats(self, in_channels, out_channels, lorder, hidden_size, repeats=1):\n",
    "        repeats = [\n",
    "            UniDeepFsmn(in_channels, out_channels, lorder, hidden_size)\n",
    "            for i in range(repeats)\n",
    "        ]\n",
    "        return nn.Sequential(*repeats)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        *,\n",
    "        mask = None\n",
    "    ):\n",
    "        ii = 0\n",
    "        for flash in self.layers:\n",
    "            #x_residual = x\n",
    "            x = flash(x, mask = mask)\n",
    "            x = self.fsmn[ii](x)\n",
    "            #x = x + x_residual\n",
    "            ii = ii + 1\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder_FLASH_DualA_FSMN(nn.Module):\n",
    "    \"\"\"This class implements the transformer encoder.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_layers : int\n",
    "        Number of transformer layers to include.\n",
    "    nhead : int\n",
    "        Number of attention heads.\n",
    "    d_ffn : int\n",
    "        Hidden size of self-attention Feed Forward layer.\n",
    "    d_model : int\n",
    "        The dimension of the input embedding.\n",
    "    kdim : int\n",
    "        Dimension for key (Optional).\n",
    "    vdim : int\n",
    "        Dimension for value (Optional).\n",
    "    dropout : float\n",
    "        Dropout for the encoder (Optional).\n",
    "    input_module: torch class\n",
    "        The module to process the source input feature to expected\n",
    "        feature dimension (Optional).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> import torch\n",
    "    >>> x = torch.rand((8, 60, 512))\n",
    "    >>> net = TransformerEncoder(1, 8, 512, d_model=512)\n",
    "    >>> output, _ = net(x)\n",
    "    >>> output.shape\n",
    "    torch.Size([8, 60, 512])\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        nhead,\n",
    "        d_ffn,\n",
    "        input_shape=None,\n",
    "        d_model=None,\n",
    "        kdim=None,\n",
    "        vdim=None,\n",
    "        dropout=0.0,\n",
    "        activation=nn.ReLU,\n",
    "        normalize_before=False,\n",
    "        causal=False,\n",
    "        attention_type=\"regularMHA\",\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.flashT = FLASHTransformer_DualA_FSMN(dim=d_model, depth=num_layers)\n",
    "        self.norm = LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        src_mask: Optional[torch.Tensor] = None,\n",
    "        src_key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        pos_embs: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ----------\n",
    "        src : tensor\n",
    "            The sequence to the encoder layer (required).\n",
    "        src_mask : tensor\n",
    "            The mask for the src sequence (optional).\n",
    "        src_key_padding_mask : tensor\n",
    "            The mask for the src keys per batch (optional).\n",
    "        \"\"\"\n",
    "        output = self.flashT(src)\n",
    "        #summary(self.flashT, [(src.size())])\n",
    "        output = self.norm(output)\n",
    "        #summary(self.norm, [(output.size())])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8bcaa",
   "metadata": {},
   "source": [
    "### one_path_flash_fsmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90eaf628",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:37.521619Z",
     "start_time": "2025-08-09T03:24:37.327793Z"
    }
   },
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "\n",
    "class ScaledSinuEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(1,))\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, device = x.shape[1], x.device\n",
    "        t = torch.arange(n, device = device).type_as(self.inv_freq)\n",
    "        sinu = einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((sinu.sin(), sinu.cos()), dim = -1)\n",
    "        return emb * self.scale\n",
    "\n",
    "class Linear(torch.nn.Module):\n",
    "    \"\"\"Computes a linear transformation y = wx + b.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    n_neurons : int\n",
    "        It is the number of output neurons (i.e, the dimensionality of the\n",
    "        output).\n",
    "    input_shape: tuple\n",
    "        It is the shape of the input tensor.\n",
    "    input_size: int\n",
    "        Size of the input tensor.\n",
    "    bias : bool\n",
    "        If True, the additive bias b is adopted.\n",
    "    combine_dims : bool\n",
    "        If True and the input is 4D, combine 3rd and 4th dimensions of input.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> inputs = torch.rand(10, 50, 40)\n",
    "    >>> lin_t = Linear(input_shape=(10, 50, 40), n_neurons=100)\n",
    "    >>> output = lin_t(inputs)\n",
    "    >>> output.shape\n",
    "    torch.Size([10, 50, 100])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_neurons,\n",
    "        input_shape=None,\n",
    "        input_size=None,\n",
    "        bias=True,\n",
    "        combine_dims=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.combine_dims = combine_dims\n",
    "\n",
    "        if input_shape is None and input_size is None:\n",
    "            raise ValueError(\"Expected one of input_shape or input_size\")\n",
    "\n",
    "        if input_size is None:\n",
    "            input_size = input_shape[-1]\n",
    "            if len(input_shape) == 4 and self.combine_dims:\n",
    "                input_size = input_shape[2] * input_shape[3]\n",
    "\n",
    "        # Weights are initialized following pytorch approach\n",
    "        self.w = nn.Linear(input_size, n_neurons, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns the linear transformation of input tensor.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "            Input to transform linearly.\n",
    "        \"\"\"\n",
    "        if x.ndim == 4 and self.combine_dims:\n",
    "            x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3])\n",
    "\n",
    "        wx = self.w(x)\n",
    "\n",
    "        return wx\n",
    "\n",
    "class GlobalLayerNorm(nn.Module):\n",
    "    \"\"\"Calculate Global Layer Normalization.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "       dim : (int or list or torch.Size)\n",
    "           Input shape from an expected input of size.\n",
    "       eps : float\n",
    "           A value added to the denominator for numerical stability.\n",
    "       elementwise_affine : bool\n",
    "          A boolean value that when set to True,\n",
    "          this module has learnable per-element affine parameters\n",
    "          initialized to ones (for weights) and zeros (for biases).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> x = torch.randn(5, 10, 20)\n",
    "    >>> GLN = GlobalLayerNorm(10, 3)\n",
    "    >>> x_norm = GLN(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, shape, eps=1e-8, elementwise_affine=True):\n",
    "        super(GlobalLayerNorm, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            if shape == 3:\n",
    "                self.weight = nn.Parameter(torch.ones(self.dim, 1))\n",
    "                self.bias = nn.Parameter(torch.zeros(self.dim, 1))\n",
    "            if shape == 4:\n",
    "                self.weight = nn.Parameter(torch.ones(self.dim, 1, 1))\n",
    "                self.bias = nn.Parameter(torch.zeros(self.dim, 1, 1))\n",
    "        else:\n",
    "            self.register_parameter(\"weight\", None)\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns the normalized tensor.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "            Tensor of size [N, C, K, S] or [N, C, L].\n",
    "        \"\"\"\n",
    "        # x = N x C x K x S or N x C x L\n",
    "        # N x 1 x 1\n",
    "        # cln: mean,var N x 1 x K x S\n",
    "        # gln: mean,var N x 1 x 1\n",
    "        if x.dim() == 3:\n",
    "            mean = torch.mean(x, (1, 2), keepdim=True)\n",
    "            var = torch.mean((x - mean) ** 2, (1, 2), keepdim=True)\n",
    "            if self.elementwise_affine:\n",
    "                x = (\n",
    "                    self.weight * (x - mean) / torch.sqrt(var + self.eps)\n",
    "                    + self.bias\n",
    "                )\n",
    "            else:\n",
    "                x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        if x.dim() == 4:\n",
    "            mean = torch.mean(x, (1, 2, 3), keepdim=True)\n",
    "            var = torch.mean((x - mean) ** 2, (1, 2, 3), keepdim=True)\n",
    "            if self.elementwise_affine:\n",
    "                x = (\n",
    "                    self.weight * (x - mean) / torch.sqrt(var + self.eps)\n",
    "                    + self.bias\n",
    "                )\n",
    "            else:\n",
    "                x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CumulativeLayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Calculate Cumulative Layer Normalization.\n",
    "\n",
    "       Arguments\n",
    "       ---------\n",
    "       dim : int\n",
    "        Dimension that you want to normalize.\n",
    "       elementwise_affine : True\n",
    "        Learnable per-element affine parameters.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> x = torch.randn(5, 10, 20)\n",
    "    >>> CLN = CumulativeLayerNorm(10)\n",
    "    >>> x_norm = CLN(x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, elementwise_affine=True):\n",
    "        super(CumulativeLayerNorm, self).__init__(\n",
    "            dim, elementwise_affine=elementwise_affine, eps=1e-8\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns the normalized tensor.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "            Tensor size [N, C, K, S] or [N, C, L]\n",
    "        \"\"\"\n",
    "        # x: N x C x K x S or N x C x L\n",
    "        # N x K x S x C\n",
    "        if x.dim() == 4:\n",
    "            x = x.permute(0, 2, 3, 1).contiguous()\n",
    "            # N x K x S x C == only channel norm\n",
    "            x = super().forward(x)\n",
    "            # N x C x K x S\n",
    "            x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        if x.dim() == 3:\n",
    "            x = torch.transpose(x, 1, 2)\n",
    "            # N x L x C == only channel norm\n",
    "            x = super().forward(x)\n",
    "            # N x C x L\n",
    "            x = torch.transpose(x, 1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "def select_norm(norm, dim, shape):\n",
    "    \"\"\"Just a wrapper to select the normalization type.\n",
    "    \"\"\"\n",
    "\n",
    "    if norm == \"gln\":\n",
    "        return GlobalLayerNorm(dim, shape, elementwise_affine=True)\n",
    "    if norm == \"cln\":\n",
    "        return CumulativeLayerNorm(dim, elementwise_affine=True)\n",
    "    if norm == \"ln\":\n",
    "        return nn.GroupNorm(1, dim, eps=1e-8)\n",
    "    else:\n",
    "        return nn.BatchNorm1d(dim)\n",
    "\n",
    "\n",
    "class SBFLASHBlock_DualA(nn.Module):\n",
    "    \"\"\"A wrapper for the SpeechBrain implementation of the transformer encoder.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    num_layers : int\n",
    "        Number of layers.\n",
    "    d_model : int\n",
    "        Dimensionality of the representation.\n",
    "    nhead : int\n",
    "        Number of attention heads.\n",
    "    d_ffn : int\n",
    "        Dimensionality of positional feed forward.\n",
    "    input_shape : tuple\n",
    "        Shape of input.\n",
    "    kdim : int\n",
    "        Dimension of the key (Optional).\n",
    "    vdim : int\n",
    "        Dimension of the value (Optional).\n",
    "    dropout : float\n",
    "        Dropout rate.\n",
    "    activation : str\n",
    "        Activation function.\n",
    "    use_positional_encoding : bool\n",
    "        If true we use a positional encoding.\n",
    "    norm_before: bool\n",
    "        Use normalization before transformations.\n",
    "\n",
    "    Example\n",
    "    ---------\n",
    "    >>> x = torch.randn(10, 100, 64)\n",
    "    >>> block = SBTransformerBlock(1, 64, 8)\n",
    "    >>> x = block(x)\n",
    "    >>> x.shape\n",
    "    torch.Size([10, 100, 64])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        d_ffn=2048,\n",
    "        input_shape=None,\n",
    "        kdim=None,\n",
    "        vdim=None,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        use_positional_encoding=False,\n",
    "        norm_before=False,\n",
    "        attention_type=\"regularMHA\",\n",
    "    ):\n",
    "\n",
    "        super(SBFLASHBlock_DualA, self).__init__()\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            activation = nn.ReLU\n",
    "        elif activation == \"gelu\":\n",
    "            activation = nn.GELU\n",
    "        else:\n",
    "            raise ValueError(\"unknown activation\")\n",
    "\n",
    "\n",
    "        self.mdl = TransformerEncoder_FLASH_DualA_FSMN(\n",
    "            num_layers=num_layers,\n",
    "            nhead=nhead,\n",
    "            d_ffn=d_ffn,\n",
    "            input_shape=input_shape,\n",
    "            d_model=d_model,\n",
    "            kdim=kdim,\n",
    "            vdim=vdim,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            normalize_before=norm_before,\n",
    "            attention_type=attention_type,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns the transformed output.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "            Tensor shape [B, L, N],\n",
    "            where, B = Batchsize,\n",
    "                   L = time points\n",
    "                   N = number of filters\n",
    "\n",
    "        \"\"\"\n",
    "        output = self.mdl(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Just a wrapper to get the activation functions.\n",
    "    \"\"\"\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "\n",
    "class Dual_Computation_Block(nn.Module):\n",
    "    \"\"\"Computation block for dual-path processing.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    intra_mdl : torch.nn.module\n",
    "        Model to process within the chunks.\n",
    "     inter_mdl : torch.nn.module\n",
    "        Model to process across the chunks.\n",
    "     out_channels : int\n",
    "        Dimensionality of inter/intra model.\n",
    "     norm : str\n",
    "        Normalization type.\n",
    "     skip_around_intra : bool\n",
    "        Skip connection around the intra layer.\n",
    "     linear_layer_after_inter_intra : bool\n",
    "        Linear layer or not after inter or intra.\n",
    "\n",
    "    Example\n",
    "    ---------\n",
    "        >>> intra_block = SBTransformerBlock(1, 64, 8)\n",
    "        >>> inter_block = SBTransformerBlock(1, 64, 8)\n",
    "        >>> dual_comp_block = Dual_Computation_Block(intra_block, inter_block, 64)\n",
    "        >>> x = torch.randn(10, 64, 100, 10)\n",
    "        >>> x = dual_comp_block(x)\n",
    "        >>> x.shape\n",
    "        torch.Size([10, 64, 100, 10])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        intra_mdl,\n",
    "        out_channels,\n",
    "        norm=\"ln\",\n",
    "        skip_around_intra=True,\n",
    "        linear_layer_after_inter_intra=True,\n",
    "    ):\n",
    "        super(Dual_Computation_Block, self).__init__()\n",
    "\n",
    "        self.intra_mdl = intra_mdl\n",
    "        self.skip_around_intra = skip_around_intra\n",
    "        self.linear_layer_after_inter_intra = linear_layer_after_inter_intra\n",
    "\n",
    "        # Norm\n",
    "        self.norm = norm\n",
    "        if norm is not None:\n",
    "            self.intra_norm = select_norm(norm, out_channels, 3)\n",
    "\n",
    "        # Linear\n",
    "        if linear_layer_after_inter_intra:\n",
    "            self.intra_linear = Linear(\n",
    "                    out_channels, input_size=out_channels\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns the output tensor.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of dimension [B, N, K, S].\n",
    "\n",
    "\n",
    "        Return\n",
    "        ---------\n",
    "        out: torch.Tensor\n",
    "            Output tensor of dimension [B, N, K, S].\n",
    "            where, B = Batchsize,\n",
    "               N = number of filters\n",
    "               K = time points in each chunk\n",
    "               S = the number of chunks\n",
    "        \"\"\"\n",
    "        B, N, S = x.shape\n",
    "        # intra RNN\n",
    "        # [B, S, N]\n",
    "        intra = x.permute(0, 2, 1).contiguous() #.view(B, S, N)\n",
    "\n",
    "        intra = self.intra_mdl(intra)\n",
    "\n",
    "        # [B, S, N]\n",
    "        if self.linear_layer_after_inter_intra:\n",
    "            intra = self.intra_linear(intra)\n",
    "\n",
    "        # [B, N, S]\n",
    "        intra = intra.permute(0, 2, 1).contiguous()\n",
    "        if self.norm is not None:\n",
    "            intra = self.intra_norm(intra)\n",
    "\n",
    "        # [B, N, S]\n",
    "        if self.skip_around_intra:\n",
    "            intra = intra + x\n",
    "\n",
    "        # inter RNN\n",
    "        # [B, S, N]\n",
    "        '''\n",
    "        inter = intra.permute(0, 2, 1).contiguous() #.view(B, S, N)\n",
    "        # [BK, S, H]\n",
    "        inter = self.inter_mdl(inter)\n",
    "\n",
    "        # [BK, S, N]\n",
    "        if self.linear_layer_after_inter_intra:\n",
    "            inter = self.inter_linear(inter)\n",
    "\n",
    "        # [B, N, S]\n",
    "        inter = inter.permute(0, 2, 1).contiguous()\n",
    "        if self.norm is not None:\n",
    "            inter = self.inter_norm(inter)\n",
    "        # [B, N, K, S]\n",
    "        out = inter + intra\n",
    "        '''\n",
    "        out = intra\n",
    "        return out\n",
    "\n",
    "\n",
    "class Dual_Path_Model(nn.Module):\n",
    "    \"\"\"The dual path model which is the basis for dualpathrnn, sepformer, dptnet.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    in_channels : int\n",
    "        Number of channels at the output of the encoder.\n",
    "    out_channels : int\n",
    "        Number of channels that would be inputted to the intra and inter blocks.\n",
    "    intra_model : torch.nn.module\n",
    "        Model to process within the chunks.\n",
    "    inter_model : torch.nn.module\n",
    "        model to process across the chunks,\n",
    "    num_layers : int\n",
    "        Number of layers of Dual Computation Block.\n",
    "    norm : str\n",
    "        Normalization type.\n",
    "    K : int\n",
    "        Chunk length.\n",
    "    num_spks : int\n",
    "        Number of sources (speakers).\n",
    "    skip_around_intra : bool\n",
    "        Skip connection around intra.\n",
    "    linear_layer_after_inter_intra : bool\n",
    "        Linear layer after inter and intra.\n",
    "    use_global_pos_enc : bool\n",
    "        Global positional encodings.\n",
    "    max_length : int\n",
    "        Maximum sequence length.\n",
    "\n",
    "    Example\n",
    "    ---------\n",
    "    >>> intra_block = SBTransformerBlock(1, 64, 8)\n",
    "    >>> inter_block = SBTransformerBlock(1, 64, 8)\n",
    "    >>> dual_path_model = Dual_Path_Model(64, 64, intra_block, inter_block, num_spks=2)\n",
    "    >>> x = torch.randn(10, 64, 2000)\n",
    "    >>> x = dual_path_model(x)\n",
    "    >>> x.shape\n",
    "    torch.Size([2, 10, 64, 2000])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        intra_model,\n",
    "        #inter_model,\n",
    "        num_layers=1,\n",
    "        norm=\"ln\",\n",
    "        K=200,\n",
    "        num_spks=2,\n",
    "        skip_around_intra=True,\n",
    "        linear_layer_after_inter_intra=True,\n",
    "        use_global_pos_enc=True,\n",
    "        max_length=20000,\n",
    "    ):\n",
    "        super(Dual_Path_Model, self).__init__()\n",
    "        self.K = K\n",
    "        self.num_spks = num_spks\n",
    "        self.num_layers = num_layers\n",
    "        # self.norm = select_norm(norm, in_channels, 3)\n",
    "        # self.conv1d_encoder = nn.Conv1d(in_channels, out_channels, 1, bias=False)\n",
    "        self.use_global_pos_enc = use_global_pos_enc\n",
    "\n",
    "        if self.use_global_pos_enc:\n",
    "            self.pos_enc = ScaledSinuEmbedding(out_channels)\n",
    "\n",
    "        self.dual_mdl = nn.ModuleList([])\n",
    "        for i in range(num_layers):\n",
    "            self.dual_mdl.append(\n",
    "                copy.deepcopy(\n",
    "                    Dual_Computation_Block(\n",
    "                        intra_model,\n",
    "                        #inter_model,\n",
    "                        out_channels,\n",
    "                        norm,\n",
    "                        skip_around_intra=skip_around_intra,\n",
    "                        linear_layer_after_inter_intra=linear_layer_after_inter_intra,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.conv1d_out = nn.Conv1d(\n",
    "            out_channels, out_channels * num_spks, kernel_size=1\n",
    "        )\n",
    "        self.conv1_decoder = nn.Conv1d(out_channels, in_channels, 1, bias=False)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.activation = nn.ReLU()\n",
    "        # gated output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, out_channels, 1), nn.Tanh()\n",
    "        )\n",
    "        self.output_gate = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, out_channels, 1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns the output tensor.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of dimension [B, N, L].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : torch.Tensor\n",
    "            Output tensor of dimension [spks, B, N, L]\n",
    "            where, spks = Number of speakers\n",
    "               B = Batchsize,\n",
    "               N = number of filters\n",
    "               L = the number of time points\n",
    "        \"\"\"\n",
    "\n",
    "        # before each line we indicate the shape after executing the line\n",
    "\n",
    "        # # [B, N, L]\n",
    "        # x = self.norm(x)\n",
    "\n",
    "        # # [B, N, L]\n",
    "        # x = self.conv1d_encoder(x)\n",
    "\n",
    "        if self.use_global_pos_enc:\n",
    "            base = x\n",
    "            x = x.transpose(1, -1)\n",
    "            emb = self.pos_enc(x)\n",
    "            emb = emb.transpose(0, -1) \n",
    "            x = base + emb\n",
    "        \n",
    "        # [B, N, S]\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dual_mdl[i](x)\n",
    "        x = self.prelu(x)\n",
    "\n",
    "        # [B, N*spks, K, S]\n",
    "        x = self.conv1d_out(x)\n",
    "        B, _, S = x.shape\n",
    "\n",
    "        # [B*spks, N, K, S]\n",
    "        x = x.view(B * self.num_spks, -1, S)\n",
    "\n",
    "        # [B*spks, N, L]\n",
    "        x = self.output(x) * self.output_gate(x)\n",
    "\n",
    "        # [B*spks, N, L]\n",
    "        x = self.conv1_decoder(x)\n",
    "\n",
    "        # [B, spks, N, L]\n",
    "        _, N, L = x.shape\n",
    "        x = x.view(B, self.num_spks, N, L)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # [spks, B, N, L]\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _padding(self, input, K):\n",
    "        \"\"\"Padding the audio times.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        K : int\n",
    "            Chunks of length.\n",
    "        P : int\n",
    "            Hop size.\n",
    "        input : torch.Tensor\n",
    "            Tensor of size [B, N, L].\n",
    "            where, B = Batchsize,\n",
    "                   N = number of filters\n",
    "                   L = time points\n",
    "        \"\"\"\n",
    "        B, N, L = input.shape\n",
    "        P = K // 2\n",
    "        gap = K - (P + L % K) % K\n",
    "        if gap > 0:\n",
    "            pad = torch.Tensor(torch.zeros(B, N, gap)).type(input.type())\n",
    "            input = torch.cat([input, pad], dim=2)\n",
    "\n",
    "        _pad = torch.Tensor(torch.zeros(B, N, P)).type(input.type())\n",
    "        input = torch.cat([_pad, input, _pad], dim=2)\n",
    "\n",
    "        return input, gap\n",
    "\n",
    "    def _Segmentation(self, input, K):\n",
    "        \"\"\"The segmentation stage splits\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        K : int\n",
    "            Length of the chunks.\n",
    "        input : torch.Tensor\n",
    "            Tensor with dim [B, N, L].\n",
    "\n",
    "        Return\n",
    "        -------\n",
    "        output : torch.tensor\n",
    "            Tensor with dim [B, N, K, S].\n",
    "            where, B = Batchsize,\n",
    "               N = number of filters\n",
    "               K = time points in each chunk\n",
    "               S = the number of chunks\n",
    "               L = the number of time points\n",
    "        \"\"\"\n",
    "        B, N, L = input.shape\n",
    "        P = K // 2\n",
    "        input, gap = self._padding(input, K)\n",
    "        # [B, N, K, S]\n",
    "        input1 = input[:, :, :-P].contiguous().view(B, N, -1, K)\n",
    "        input2 = input[:, :, P:].contiguous().view(B, N, -1, K)\n",
    "        input = (\n",
    "            torch.cat([input1, input2], dim=3).view(B, N, -1, K).transpose(2, 3)\n",
    "        )\n",
    "\n",
    "        return input.contiguous(), gap\n",
    "\n",
    "    def _over_add(self, input, gap):\n",
    "        \"\"\"Merge the sequence with the overlap-and-add method.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        input : torch.tensor\n",
    "            Tensor with dim [B, N, K, S].\n",
    "        gap : int\n",
    "            Padding length.\n",
    "\n",
    "        Return\n",
    "        -------\n",
    "        output : torch.tensor\n",
    "            Tensor with dim [B, N, L].\n",
    "            where, B = Batchsize,\n",
    "               N = number of filters\n",
    "               K = time points in each chunk\n",
    "               S = the number of chunks\n",
    "               L = the number of time points\n",
    "\n",
    "        \"\"\"\n",
    "        B, N, K, S = input.shape\n",
    "        P = K // 2\n",
    "        # [B, N, S, K]\n",
    "        input = input.transpose(2, 3).contiguous().view(B, N, -1, K * 2)\n",
    "\n",
    "        input1 = input[:, :, :, :K].contiguous().view(B, N, -1)[:, :, P:]\n",
    "        input2 = input[:, :, :, K:].contiguous().view(B, N, -1)[:, :, :-P]\n",
    "        input = input1 + input2\n",
    "        # [B, N, L]\n",
    "        if gap > 0:\n",
    "            input = input[:, :, :-gap]\n",
    "\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fddcdf",
   "metadata": {},
   "source": [
    "## Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6556acb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:37.726923Z",
     "start_time": "2025-08-09T03:24:37.523348Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model = 256, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)].to(x.device)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64c6ac70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:37.909858Z",
     "start_time": "2025-08-09T03:24:37.728649Z"
    }
   },
   "outputs": [],
   "source": [
    "class Fusion(nn.Module):    \n",
    "    def __init__(self, N, B):\n",
    "        super(Fusion, self).__init__()               \n",
    "        \n",
    "        self.layer_norm = nn.GroupNorm(1, N, eps=1e-8)\n",
    "        # [M, N, K] -> [M, N, K] \n",
    "        self.bottleneck_conv1x1 = nn.Conv1d(N, N, 1, bias=False)\n",
    "        # [M, N, K] -> [M, N, K]         \n",
    "        self.po_encoding = PositionalEncoding(d_model=256)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model=256, nhead=4, dim_feedforward=64*4)\n",
    "        self.eeg_net = TransformerEncoder(encoder_layers, num_layers=5)\n",
    "        self.fusion = nn.Conv1d(N+2*B,N, 1, bias=False)\n",
    "        \n",
    "    def forward(self, x, eeg, reference, speech):\n",
    "        mixture_w = x\n",
    "\n",
    "        #print(x.shape)\n",
    "        M, N, D = x.size()\n",
    "        \n",
    "        x = self.layer_norm(x) # [M, N, K]  \n",
    "    \n",
    "        x = self.bottleneck_conv1x1(x) # [M, N, K]\n",
    "\n",
    "        eeg = self.po_encoding(eeg.transpose(0,1))  #[B,C,T] -> [C,B,T]\n",
    "\n",
    "        eeg = self.eeg_net(eeg)\n",
    "\n",
    "        eeg = eeg.transpose(0,1).transpose(1,2)\n",
    "\n",
    "        eeg = F.interpolate(eeg, (D), mode='linear')\n",
    "\n",
    "        x = torch.cat((x, eeg),1)\n",
    "        \n",
    "        fused_output  = self.fusion(x)\n",
    "        \n",
    "        return fused_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9201ad9",
   "metadata": {},
   "source": [
    "## Seprator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fac2ab3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:38.274808Z",
     "start_time": "2025-08-09T03:24:38.093390Z"
    }
   },
   "outputs": [],
   "source": [
    "class Separator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Separator, self).__init__()\n",
    "        \n",
    "        # mossformer 2\n",
    "        intra_model = SBFLASHBlock_DualA(\n",
    "            num_layers = Config.intra_numlayers,\n",
    "            d_model = Config.encoder_out_nchannels,\n",
    "            nhead = Config.intra_nhead,\n",
    "            d_ffn = Config.intra_dffn,\n",
    "            dropout = Config.intra_dropout,\n",
    "            use_positional_encoding = Config.intra_use_positional,                 \n",
    "            norm_before = Config.intra_norm_before                              \n",
    "        )\n",
    "\n",
    "        self.masknet = Dual_Path_Model(\n",
    "            in_channels = Config.encoder_out_nchannels,\n",
    "            out_channels = Config.encoder_out_nchannels,\n",
    "            intra_model = intra_model,\n",
    "            num_layers = Config.masknet_numlayers,\n",
    "            norm = Config.masknet_norm,\n",
    "            K = Config.masknet_chunksize,\n",
    "            num_spks = Config.masknet_numspks,\n",
    "            skip_around_intra = Config.masknet_extraskipconnection,\n",
    "            linear_layer_after_inter_intra = Config.masknet_useextralinearlayer\n",
    "        )\n",
    "\n",
    "        # reference\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Keep this API same with TasNet\n",
    "        \n",
    "            mixture_w: [M, N, K], M is batch size\n",
    "        returns:\n",
    "            est_mask: [M, C, N, K]\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.masknet(x)\n",
    "\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def overlap_and_add(signal, frame_step):\n",
    "    \"\"\"Reconstructs a signal from a framed representation.\n",
    "\n",
    "    Adds potentially overlapping frames of a signal with shape\n",
    "    `[..., frames, frame_length]`, offsetting subsequent frames by `frame_step`.\n",
    "    The resulting tensor has shape `[..., output_size]` where\n",
    "\n",
    "        output_size = (frames - 1) * frame_step + frame_length\n",
    "\n",
    "        signal: A [..., frames, frame_length] Tensor. All dimensions may be unknown, and rank must be at least 2.\n",
    "        frame_step: An integer denoting overlap offsets. Must be less than or equal to frame_length.\n",
    "\n",
    "    Returns:\n",
    "        A Tensor with shape [..., output_size] containing the overlap-added frames of signal's inner-most two dimensions.\n",
    "        output_size = (frames - 1) * frame_step + frame_length\n",
    "\n",
    "    Based on https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/signal/python/ops/reconstruction_ops.py\n",
    "    \"\"\"\n",
    "    outer_dimensions = signal.size()[:-2]\n",
    "    frames, frame_length = signal.size()[-2:]\n",
    "\n",
    "    subframe_length = math.gcd(frame_length, frame_step)  # gcd=Greatest Common Divisor\n",
    "    subframe_step = frame_step // subframe_length\n",
    "    subframes_per_frame = frame_length // subframe_length\n",
    "    output_size = frame_step * (frames - 1) + frame_length\n",
    "    output_subframes = output_size // subframe_length\n",
    "\n",
    "    subframe_signal = signal.view(*outer_dimensions, -1, subframe_length)\n",
    "\n",
    "    frame = torch.arange(0, output_subframes).unfold(0, subframes_per_frame, subframe_step)\n",
    "    frame = frame.clone().detach().cuda()  # signal may in GPU or CPU\n",
    "    frame = frame.contiguous().view(-1)\n",
    "\n",
    "    result = signal.new_zeros(*outer_dimensions, output_subframes, subframe_length)\n",
    "    result.index_add_(-2, frame, subframe_signal)\n",
    "    result = result.view(*outer_dimensions, -1)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a7a63",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cf55aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:38.638560Z",
     "start_time": "2025-08-09T03:24:38.457424Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,N,L):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_channels=N, out_channels=10, kernel_size=L, stride=L//2)\n",
    "        self.conv1d = nn.Conv1d(10,1,1,bias=False)\n",
    "\n",
    "    def forward(self, mixture_w, est_mask):\n",
    "        x = mixture_w * est_mask      #torch.Size([batchsize, 256, 1599])\n",
    "\n",
    "        x = self.conv(x) #[batchsize, 256, 1599]\n",
    "\n",
    "        output = x.contiguous()  # B*C, 1, L\n",
    "\n",
    "        output = self.conv1d(output)\n",
    "\n",
    "        return output  # remove extra dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021ae70",
   "metadata": {},
   "source": [
    "## Mossformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1378ce19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T03:24:39.223625Z",
     "start_time": "2025-08-09T03:24:39.015761Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "\n",
    "class Mossformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mossformer, self).__init__()\n",
    "        \n",
    "        self.N, self.L = Config.encoder_out_nchannels, Config.encoder_kernel_size\n",
    "        self.B, self.K= Config.emb_size,Config.masknet_chunksize\n",
    "        self.device = Config.device\n",
    "                                        \n",
    "        self.eegencoder = EEGEncoder()\n",
    "        \n",
    "        self.audioencoder = AudioEncoder(self.L, self.N)\n",
    "        \n",
    "        self.fusion = Fusion(self.N, self.B)\n",
    "        \n",
    "        self.separator = Separator()\n",
    "        \n",
    "        self.decoder = Decoder(self.N, self.L)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_normal_(p)\n",
    "\n",
    "    def forward(self, mixture, eeg, feature,reference=None):\n",
    "        \"\"\"\n",
    "            mixture: [M, T], M is batch size, T is #samples\n",
    "        Returns:\n",
    "            est_source: [M, C, T]\n",
    "        \"\"\"\n",
    "        mixture = mixture.to(self.device)\n",
    "        \n",
    "        eeg = eeg.to(self.device)\n",
    "        \n",
    "        feature = feature.to(self.device)\n",
    "        \n",
    "        eeg = self.eegencoder(eeg,feature)\n",
    "        \n",
    "        mixture_w = self.audioencoder(mixture)\n",
    "        \n",
    "        fused_output = self.fusion(mixture_w, eeg, reference, mixture)\n",
    "        \n",
    "        est_mask = self.separator(fused_output)\n",
    "        \n",
    "        est_source = self.decoder(mixture_w, est_mask)\n",
    "              \n",
    "        \n",
    "        T_origin = mixture.size(-1)\n",
    "        T_conv = est_source.size(-1)\n",
    "        est_source = F.pad(est_source, (0, T_origin - T_conv))\n",
    "        return est_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b290bf",
   "metadata": {},
   "source": [
    "# Pre-Training(Single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83931b86",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-09T03:24:16.761Z"
    }
   },
   "outputs": [],
   "source": [
    "train_model(Train_dataloader, Valid_dataloader, start_epoch = 0, additional_epochs = 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "434.664px",
    "left": "228px",
    "top": "178.141px",
    "width": "257px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
